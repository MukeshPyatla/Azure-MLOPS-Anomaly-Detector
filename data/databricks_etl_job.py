# src/data/databricks_etl_job.py
# This script is designed to run as a Databricks job

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, hour, to_timestamp, expr
from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType, TimestampType, IntegerType

# --- Configuration Variables (will be passed as job parameters in Databricks) ---
RAW_BLOB_PATH = "/mnt/raw_transactions_data" # Mount point for raw data container
PROCESSED_BLOB_PATH = "/mnt/processed_transactions_data" # Mount point for processed data container
# --- End Configuration ---

# Initialize Spark Session (Databricks provides this context)
spark = SparkSession.builder.appName("AnomalyDetectionETL").getOrCreate()

print(f"Reading raw data from: {RAW_BLOB_PATH}")
print(f"Writing processed data to: {PROCESSED_BLOB_PATH}")

# Define the schema of the *inner JSON message* within the Avro records
# This is based on the data generated by your data_generator.py
transaction_schema = StructType([
    StructField("transaction_id", StringType(), True),
    StructField("user_id", StringType(), True),
    StructField("amount", DoubleType(), True),
    StructField("timestamp", StringType(), True),
    StructField("is_fraud", BooleanType(), True),
    StructField("ip_address", StringType(), True),
    StructField("device_type", StringType(), True),
    StructField("merchant_id", StringType(), True)
])

# 1. Read Raw Avro Data from Blob Storage
# Event Hubs Capture writes Avro files. Spark can read Avro directly.
# We need to extract the 'Body' column which contains the actual JSON message.
raw_df = spark.read.format("avro").load(RAW_BLOB_PATH)

# 2. Extract JSON string from 'Body' column and parse it
# 'Body' column from Event Hubs Capture Avro is binary. Convert to string, then parse JSON.
df = raw_df.withColumn("json_body", raw_df["Body"].cast("string")) \
           .withColumn("data", from_json(col("json_body"), transaction_schema)) \
           .select("data.*") # Select all fields from the parsed 'data' struct

# Add error handling or schema evolution logic if needed for production

# 3. Apply Transformations and Feature Engineering
df_transformed = df.withColumn("timestamp_utc", to_timestamp(col("timestamp"), "yyyy-MM-dd'T'HH:mm:ss.SSSSSS")) \
                   .withColumn("transaction_hour", hour(col("timestamp_utc"))) \
                   .select(
                       "transaction_id",
                       "user_id",
                       "amount",
                       "timestamp", # Keep original string
                       "timestamp_utc",
                       "transaction_hour",
                       "is_fraud",
                       "ip_address",
                       "device_type",
                       "merchant_id"
                   )

# Add any other specific feature engineering logic here
# Example: You could calculate rolling averages, user velocity, etc., if you had a larger dataset or joined with other tables.

# 4. Write Processed Data to Azure Blob Storage in Parquet format
# Partition by transaction_hour for efficient querying later
print(f"Writing processed data to: {PROCESSED_BLOB_PATH}")
# Use 'overwrite' for development if you want to re-run and replace, or 'append' for continuous flow
df_transformed.write.partitionBy("transaction_hour").mode("append").parquet(PROCESSED_BLOB_PATH)

print("Data processing and feature engineering job completed.")